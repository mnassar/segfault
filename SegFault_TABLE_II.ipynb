{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegFault -- TABLE II",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnassar/segfault/blob/main/SegFault_TABLE_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzPrYwb4tKc"
      },
      "source": [
        "# Segmentation Fault: A cheap defense against adversarial machine learning\n",
        "\n",
        "## Author: Mohamed Nassar, Doha Al Bared\n",
        "\n",
        "## Off-The-Shelf Classifiers Detection Auc For The Different IQR Representations\n",
        "## Table II\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSB8pNMgIP8T"
      },
      "source": [
        "# install foolbox for generating adversarial samples\n",
        "# better to run it first since it requires runtime restart\n",
        "!pip install foolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qwaFdFZnRXC"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import foolbox as fb\n",
        "\n",
        "# classifiers \n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "#tf.debugging.set_log_device_placement(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZZUJlt7Iok"
      },
      "source": [
        "# Classifier and dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp-4XBRnmK4M"
      },
      "source": [
        "# https://drive.google.com/file/d/1H4KEE0Vp8DFZOe_QfcxqOxEVnpun-uka/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1H4KEE0Vp8DFZOe_QfcxqOxEVnpun-uka' -O CIFAR10model.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdWfAkkn4mPG"
      },
      "source": [
        "Load the target CIFAR classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIlTauzf4iao"
      },
      "source": [
        "# load the cifar classifier\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "pretrained_model = load_model('CIFAR10model.h5')\n",
        "pretrained_model.trainable = False\n",
        "pretrained_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzy0CZ7mRjA"
      },
      "source": [
        "Load the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FjbwFv3hRHW"
      },
      "source": [
        "#get dataset: cifar10\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.datasets import cifar10\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "print(\"-------------\")\n",
        "print (ds_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAfESwHPn0gU"
      },
      "source": [
        "Generate IQR values for our dataset for original and adversarial images\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo2P897Qaus_"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxw2BRaJbunj"
      },
      "source": [
        "# normalize images \n",
        "\n",
        "# these are the numbers used during training the model \n",
        "mean = 120.70748\n",
        "std = 64.150024\n",
        "bound_min = (0-mean)/std\n",
        "bound_max = (255-mean)/std\n",
        "BATCH_SIZE=128\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  # return tf.cast(image, tf.float32) / 255., tf.one_hot(label, 10)\n",
        "  return (tf.cast(image, tf.float32) - mean) / std, tf.one_hot(label, 10)\n",
        "\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CucBP64Ttyjd"
      },
      "source": [
        "pretrained_model.evaluate(iter(ds_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUpNgveC2Aqm"
      },
      "source": [
        "\n",
        "pretrained_model.evaluate(iter(ds_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxSRUEI9lW6"
      },
      "source": [
        "# Data set preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TXVbeZXVYvX"
      },
      "source": [
        "## Choose Number of batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjH1S8qmYuJl"
      },
      "source": [
        "NB_BATCHES = 30 # means that we will have NB_BATCHES radnom normal batches\n",
        "# and NB_BATCHES random adversarial batches each coming from a different normal batch and a different epsilon "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P26SR_0TVV5S"
      },
      "source": [
        "## Choose attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ9TI26lVTuB"
      },
      "source": [
        "# attack = fb.attacks.L2CarliniWagnerAttack()\n",
        "# attack = fb.attacks.PGD()\n",
        "attack = fb.attacks.FGSM()\n",
        "# fb.attacks.LinfDeepFoolAttack()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul0V6NZbG3lh"
      },
      "source": [
        "ds_experiment = []\n",
        "ds_experiment_f = [] \n",
        "epsilons = [0.02, 0.06, 0.1]\n",
        "\n",
        "fmodel = fb.models.TensorFlowModel(model=pretrained_model, bounds=(bound_min, bound_max))\n",
        "\n",
        "\n",
        "gen = iter(ds_train)\n",
        "\n",
        "for b in range(NB_BATCHES): \n",
        "  images, labels = gen.next()\n",
        "  labels_class = tf.argmax(labels, axis=1)\n",
        "  ds_experiment.append(images) \n",
        "  raw, fimages, is_adv = attack(fmodel, images, criterion=fb.criteria.Misclassification(labels_class),epsilons=epsilons[b%3])\n",
        "  ds_experiment_f.append(fimages)\n",
        "\n",
        "\n",
        "# for b in range(NB_BATCHES//3):\n",
        "#   for eps in epsilons:\n",
        "#     images, labels = gen.next() \n",
        "#     labels_class = tf.argmax(labels, axis=1)\n",
        "#     raw, fimages, is_adv = attack(fmodel, images, criterion=fb.criteria.Misclassification(labels_class),epsilons=eps)\n",
        "#     ds_experiment.append(fimages)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTUE6Lr_sKz4"
      },
      "source": [
        "\n",
        "ds_experiment += ds_experiment_f\n",
        "len(ds_experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjv6jKQi9zEX"
      },
      "source": [
        "# IQR calculations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw_iN6OKVOUZ"
      },
      "source": [
        "from keras import backend as K\n",
        "# pretrained_model.summary()\n",
        "NB_LAYERS=20\n",
        "NB_NODES_PER_LAYER=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ko2kWRVvNa"
      },
      "source": [
        "inp = pretrained_model.input  \n",
        "layers_= pretrained_model.layers[-NB_LAYERS:]\n",
        "outputs = [lay.output for lay in layers_]     \n",
        "intermediate_model = K.function([inp], outputs) \n",
        "\n",
        "\n",
        "# select NB_NODES_PER_LAYER random nodes from each selected layer\n",
        "print (\"these nodes will be used to compute the IQR-\"+str(NB_LAYERS*NB_NODES_PER_LAYER))\n",
        "\n",
        "node_indices=[]\n",
        "for lay in layers_[:-1]:\n",
        "  # we omit the first dim (batch dim) of each layer \n",
        "  node_indices.append([[np.random.randint(0,d) for d in lay.output.shape[1:]] for s in range(NB_NODES_PER_LAYER)])\n",
        "  # print(\"%s:\" % lay.name)\n",
        "  \n",
        "\n",
        "# add the last layer \n",
        "node_indices.append([[x] for x in range(10)])\n",
        "# print(\"%s:\" % layers_[-1].name)\n",
        "print(intermediate_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ObjWXlstSq3"
      },
      "source": [
        "\n",
        "%%time \n",
        "iqr_all = []\n",
        "iqr_10_all = [] \n",
        "iqr_4k_all =[]\n",
        "for b in range(2*NB_BATCHES):\n",
        "  print(\"batch %d:\" % b) \n",
        "  images = ds_experiment[b]\n",
        "  preds = pretrained_model.predict(images)\n",
        "  preds_value = tf.reduce_max(preds, axis=1)\n",
        "  preds_idx = tf.argmax(preds, axis=1)\n",
        "  preds_layers = intermediate_model(images)\n",
        "  preds_4k = [] \n",
        "  for u in range(NB_LAYERS): # loop through the last 10 layers ]\n",
        "    for v in node_indices[u]: # loop through the 10 random nodes for that layer \n",
        "      t = tuple(v)\n",
        "      # print((0,*t))\n",
        "      preds_4k.append ( preds_layers[u][(...,*t)] )\n",
        "  preds_4k = np.array(preds_4k).T\n",
        "  # print (preds_value)\n",
        "  # print (preds_idx)\n",
        "  # print ( preds_value == tf.gather_nd(preds, list(zip(range(BATCH_SIZE), preds_idx))) )\n",
        "  iqr = [] \n",
        "  iqr10 = [] \n",
        "  iqr4k = []\n",
        "  for i in range(32): \n",
        "    for j in range(32): \n",
        "      mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "      mask[:,i,j,:]=0 \n",
        "      images_0 = images * mask\n",
        "      preds_0 = pretrained_model.predict(images_0)\n",
        "      preds_value_0 = tf.gather_nd(preds_0, list(zip(range(BATCH_SIZE), preds_idx))) \n",
        "      # preds_value_0_old = tf.reduce_max(preds_0, axis=1)\n",
        "      # print (preds_value_0)\n",
        "      # print (preds_value_0_old == preds_value_0)\n",
        "      iqr.append(abs(preds_value - preds_value_0))  \n",
        "      iqr10.append(abs(preds - preds_0))\n",
        "      preds_layers_0 = intermediate_model(images_0)\n",
        "      preds_4k_0 = [] \n",
        "      for u in range(NB_LAYERS): # loop through the last 10 layers ]\n",
        "        for v in node_indices[u]: # loop through the 10 random nodes for that layer \n",
        "          t = tuple(v) \n",
        "          preds_4k_0.append ( preds_layers_0[u][(...,*t)] )\n",
        "      preds_4k_0 = np.array(preds_4k_0).T\n",
        "      iqr4k.append(abs(preds_4k - preds_4k_0))\n",
        "  iqr_vals = np.percentile(iqr, 75, axis=0) - np.percentile(iqr, 25, axis=0)\n",
        "  iqr_10_vals = np.percentile(iqr10, 75, axis=0) - np.percentile(iqr10, 25, axis=0)\n",
        "  iqr_4k_vals = np.percentile(iqr4k, 75, axis=0) - np.percentile(iqr4k, 25, axis=0)\n",
        "  iqr_all.append(iqr_vals)\n",
        "  iqr_10_all.append(iqr_10_vals)\n",
        "  iqr_4k_all.append(iqr_4k_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM-JrGb5Yd9c"
      },
      "source": [
        "# print (preds_4k.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao909WCwmEiQ"
      },
      "source": [
        "# IQR-1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OSIoP3aAyJc"
      },
      "source": [
        "# classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3SSMlhKF3dT"
      },
      "source": [
        "len(iqr_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmOEBfgxAxK_"
      },
      "source": [
        "X = np.array(iqr_all).flatten()\n",
        "y = np.concatenate( ( np.zeros(NB_BATCHES*BATCH_SIZE), np.ones(NB_BATCHES*BATCH_SIZE) ) )\n",
        "\n",
        "score = cross_val_score(XGBClassifier(), X.reshape(-1,1), y, cv=2)\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl-5wx11CZg8"
      },
      "source": [
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X.reshape(-1,1), y, test_size=0.2)\n",
        "rdm = RandomForestClassifier().fit(X_train,y_train)\n",
        "svc = SVC(probability=True).fit(X_train,y_train)\n",
        "xgb = XGBClassifier().fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vpVxLq3JDn5"
      },
      "source": [
        "# print(rdm.score(X_train,y_train))\n",
        "print(rdm.score(X_test,y_test))\n",
        "# print(svc.score(X_train,y_train))\n",
        "print(svc.score(X_test,y_test))\n",
        "# print(xgb.score(X_train,y_train))\n",
        "print(xgb.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDH_WtymHkIA"
      },
      "source": [
        "# AUC \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "rdm_probs = rdm.predict_proba(X_test)[:,1]\n",
        "svc_probs = svc.predict_proba(X_test)[:,1]\n",
        "xgb_probs = xgb.predict_proba(X_test)[:,1]\n",
        "\n",
        "rdm_auc = roc_auc_score(y_test, rdm_probs)\n",
        "svc_auc = roc_auc_score(y_test, svc_probs)\n",
        "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
        "\n",
        "print('Random Forest: AUROC = %.3f' %(rdm_auc) )\n",
        "print('SVC: AUROC = %.3f' %(svc_auc) )\n",
        "print('RaXGBClassifier: AUROC = %.3f' %(xgb_auc) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfT_XFMLr5w-"
      },
      "source": [
        "# IQR-10D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeFd_xX9W-ti"
      },
      "source": [
        "\n",
        "# %%time \n",
        "# iqr_all = []\n",
        "# for b in range(2*NB_BATCHES):\n",
        "#   images = ds_experiment[b]\n",
        "#   preds = pretrained_model.predict(images)\n",
        "#   iqr = [] \n",
        "#   for i in range(32): \n",
        "#     for j in range(32): \n",
        "#       mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "#       mask[:,i,j,:]=0 \n",
        "#       images_0 = images * mask\n",
        "#       preds_0 = pretrained_model.predict(images_0)\n",
        "#       iqr.append(abs(preds - preds_0))  \n",
        "#   iqr_vals = np.percentile(iqr, 75, axis=0)-np.percentile(iqr, 25, axis=0)\n",
        "#   iqr_all.append(iqr_vals)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOigF0xnvpvC"
      },
      "source": [
        "# np.array(iqr_10_all).shape\n",
        "# X = np.array(iqr_all).reshape(-1,10) \n",
        "# X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr6znq4NOIQ7"
      },
      "source": [
        "# Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L1CKLW1OJ5k"
      },
      "source": [
        "X = np.array(iqr_10_all).reshape(-1,10) \n",
        "y = np.concatenate( ( np.zeros(NB_BATCHES*BATCH_SIZE), np.ones(NB_BATCHES*BATCH_SIZE) ) )\n",
        "\n",
        "score = cross_val_score(XGBClassifier(), X, y, cv=2)\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpaNPPXEEj5n"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\n",
        "rdm = RandomForestClassifier().fit(X_train,y_train)\n",
        "svc = SVC(probability=True).fit(X_train,y_train)\n",
        "xgb = XGBClassifier().fit(X_train,y_train)\n",
        "\n",
        "print(rdm.score(X_test,y_test))\n",
        "print(svc.score(X_test,y_test))\n",
        "print(xgb.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX0ovnklFTp0"
      },
      "source": [
        "# AUC \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "rdm_probs = rdm.predict_proba(X_test)[:,1]\n",
        "svc_probs = svc.predict_proba(X_test)[:,1]\n",
        "xgb_probs = xgb.predict_proba(X_test)[:,1]\n",
        "\n",
        "rdm_auc = roc_auc_score(y_test, rdm_probs)\n",
        "svc_auc = roc_auc_score(y_test, svc_probs)\n",
        "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
        "\n",
        "print('Random Forest: AUROC = %.3f' %(rdm_auc) )\n",
        "print('SVC: AUROC = %.3f' %(svc_auc) )\n",
        "print('RaXGBClassifier: AUROC = %.3f' %(xgb_auc) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59tE40ooK0Vq"
      },
      "source": [
        "rdm_fpr, rdm_tpr, _ = roc_curve(y_test, rdm_probs)\n",
        "svc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "\n",
        "plt.plot(rdm_fpr, rdm_tpr, marker='+', label='Random Forest (AUROC = %0.3f) '% rdm_auc )\n",
        "plt.plot(svc_fpr, svc_tpr, marker='.', label='SVC (AUROC = %0.3f) '% svc_auc)\n",
        "plt.plot(xgb_fpr, xgb_tpr, marker='*', label='XGB (AUROC = %0.3f) '% xgb_auc)\n",
        "\n",
        "# Title\n",
        "plt.title('ROC Plot')\n",
        "# Axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# Show legend\n",
        "plt.legend() \n",
        "# Show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnwrdseoW_hy"
      },
      "source": [
        "# IQR-4K-D\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Pj2sEs3oDz"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_enWDu93YYo"
      },
      "source": [
        "# We try a very basic classification \n",
        "X = np.array(iqr_4k_all).reshape(-1,3810) \n",
        "y = np.concatenate( ( np.zeros(NB_BATCHES*BATCH_SIZE), np.ones(NB_BATCHES*BATCH_SIZE) ) )\n",
        "\n",
        "score = cross_val_score(XGBClassifier(), X, y, cv=2)\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct6drCk8fEQk"
      },
      "source": [
        "print (np.array(iqr_4k_all).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4SIHwW4Rwzl"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\n",
        "rdm = RandomForestClassifier().fit(X_train,y_train)\n",
        "svc = SVC(probability=True).fit(X_train,y_train)\n",
        "xgb = XGBClassifier().fit(X_train,y_train)\n",
        "\n",
        "print(rdm.score(X_test,y_test))\n",
        "print(svc.score(X_test,y_test))\n",
        "print(xgb.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu2QXoNYSagq"
      },
      "source": [
        "# AUC \n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "rdm_probs = rdm.predict_proba(X_test)[:,1]\n",
        "svc_probs = svc.predict_proba(X_test)[:,1]\n",
        "xgb_probs = xgb.predict_proba(X_test)[:,1]\n",
        "\n",
        "rdm_auc = roc_auc_score(y_test, rdm_probs)\n",
        "svc_auc = roc_auc_score(y_test, svc_probs)\n",
        "xgb_auc = roc_auc_score(y_test, xgb_probs)\n",
        "\n",
        "print('Random Forest: AUROC = %.3f' %(rdm_auc) )\n",
        "print('SVC: AUROC = %.3f' %(svc_auc) )\n",
        "print('RaXGBClassifier: AUROC = %.3f' %(xgb_auc) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lg8SZhcRx19"
      },
      "source": [
        "rdm_fpr, rdm_tpr, _ = roc_curve(y_test, rdm_probs)\n",
        "svc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "\n",
        "plt.plot(rdm_fpr, rdm_tpr, marker='+', label='Random Forest (AUROC = %0.3f) '% rdm_auc )\n",
        "plt.plot(svc_fpr, svc_tpr, marker='.', label='SVC (AUROC = %0.3f) '% svc_auc)\n",
        "plt.plot(xgb_fpr, xgb_tpr, marker='*', label='XGB (AUROC = %0.3f) '% xgb_auc)\n",
        "\n",
        "# Title\n",
        "plt.title('ROC Plot')\n",
        "# Axis labels\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "# Show legend\n",
        "plt.legend() \n",
        "# Show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}