{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SegFault -- ML-LOO Implementation",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnassar/segfault/blob/main/SegFault_ML_LOO_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGzPrYwb4tKc"
      },
      "source": [
        "# Segmentation Fault: A cheap defense against adversarial machine learning\n",
        "## ML LOO Implementation\n",
        "\n",
        "## Authors: Mohamed Nassar, Doha Al Bared\n",
        "### Department of Computer Science \n",
        "### AUB \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSB8pNMgIP8T"
      },
      "source": [
        "# install foolbox for generating adversarial samples\n",
        "# better to run it first since it requires runtime restart\n",
        "!pip install foolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyPKu8zxBTbD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qwaFdFZnRXC"
      },
      "source": [
        "import tensorflow.compat.v2 as tf\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import foolbox as fb\n",
        "\n",
        "# classifiers \n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#tf.debugging.set_log_device_placement(True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZZUJlt7Iok"
      },
      "source": [
        "# Classifier and dataset exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp-4XBRnmK4M"
      },
      "source": [
        "# https://drive.google.com/file/d/1H4KEE0Vp8DFZOe_QfcxqOxEVnpun-uka/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1H4KEE0Vp8DFZOe_QfcxqOxEVnpun-uka' -O CIFAR10model.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdWfAkkn4mPG"
      },
      "source": [
        "Load the target CIFAR classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIlTauzf4iao"
      },
      "source": [
        "# load the cifar classifier\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "pretrained_model = load_model('CIFAR10model.h5')\n",
        "pretrained_model.trainable = False\n",
        "pretrained_model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL0WjV1OBg5k"
      },
      "source": [
        "print (len ( pretrained_model.layers)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzy0CZ7mRjA"
      },
      "source": [
        "Load the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FjbwFv3hRHW"
      },
      "source": [
        "#get dataset: cifar10\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.datasets import cifar10\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'cifar10',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "\n",
        "print (ds_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAfESwHPn0gU"
      },
      "source": [
        "Generate IQR values for our dataset for original and adversarial images\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo2P897Qaus_"
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNRylcKAPymG"
      },
      "source": [
        "ds_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uxw2BRaJbunj"
      },
      "source": [
        "# normalize images \n",
        "\n",
        "# these are the numbers used during training the model \n",
        "mean = 120.70748\n",
        "std = 64.150024\n",
        "bound_min = (0-mean)/std\n",
        "bound_max = (255-mean)/std\n",
        "BATCH_SIZE=128\n",
        "\n",
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  # return tf.cast(image, tf.float32) / 255., tf.one_hot(label, 10)\n",
        "  return (tf.cast(image, tf.float32) - mean) / std, tf.one_hot(label, 10)\n",
        "\n",
        "\n",
        "ds_train = ds_train.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.cache()\n",
        "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
        "ds_train = ds_train.batch(BATCH_SIZE)\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_test.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.batch(BATCH_SIZE)\n",
        "ds_test = ds_test.cache()\n",
        "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CucBP64Ttyjd"
      },
      "source": [
        "# training accuracy \n",
        "pretrained_model.evaluate(iter(ds_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUpNgveC2Aqm"
      },
      "source": [
        "# testing accuracy \n",
        "pretrained_model.evaluate(iter(ds_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wFS_6bdRNA"
      },
      "source": [
        "# plot examples from original data \n",
        "\n",
        "# images, labels = next(iter(ds_train))\n",
        "# print (images.shape)\n",
        "# image = np.squeeze(images[0], axis=-1)\n",
        "# for (img, label) in \n",
        "# with tf.device(\"/gpu:0\"):\n",
        "images, labels = next(iter(ds_train))\n",
        "\n",
        "preds_imgs = pretrained_model.predict(images)\n",
        "for img, label, pred in zip(images, labels, preds_imgs): \n",
        "  img = img.numpy()\n",
        "  label = label.numpy()\n",
        "  plt.figure(figsize = (1,1))\n",
        "  plt.axis('off')\n",
        "  plt.imshow((img * std + mean).astype(np.uint8))\n",
        "  plt.show()\n",
        "  print (class_names[np.argmax(label)], class_names[np.argmax(pred)])\n",
        "\n",
        "  if input()==\"q\": \n",
        "    break "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFF3_8xX6__z"
      },
      "source": [
        "# LOO: Leave One Out - IQR Calculations for original \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIfcLENJZIOs"
      },
      "source": [
        "\n",
        "# take a batch\n",
        "images, labels = next(iter(ds_train))\n",
        "# print (images.shape)\n",
        "preds = pretrained_model.predict(images)\n",
        "preds_class = tf.argmax(preds, axis=1)\n",
        "labels = tf.argmax(labels, axis=1)\n",
        "# performance eval for one batch\n",
        "# to make sure we have acceptable classification accuracy \n",
        "print (\"accuracy\", tf.reduce_mean( tf.cast( tf.equal(labels, preds_class), tf.float32 ) ).numpy().item() * 100, \"%\")  \n",
        "\n",
        "\n",
        "# prediction values \n",
        "preds_value = tf.reduce_max(preds, axis=1)\n",
        "print(preds_value.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdUQ-4ER8voj"
      },
      "source": [
        "%%time\n",
        "iqr = [] \n",
        "\n",
        "for i in range(32): \n",
        "  for j in range(32): \n",
        "    mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "    mask[:,i,j,:]=0 \n",
        "    images_0 = images * mask\n",
        "    preds_0 = pretrained_model.predict(images_0)\n",
        "    preds_value_0 = tf.reduce_max(preds_0, axis=1)\n",
        "    iqr.append(abs(preds_value - preds_value_0))\n",
        "\n",
        "iqr = np.array(iqr)\n",
        "print(iqr.shape)  \n",
        "  # print(iqr.device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX9WUuOLmY-q"
      },
      "source": [
        "\n",
        "iqr_vals = np.percentile(iqr, 75, axis=0)-np.percentile(iqr, 25, axis=0)\n",
        "print (iqr_vals.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnRn5EZSBxDu"
      },
      "source": [
        "plt.scatter(range(BATCH_SIZE), iqr_vals)\n",
        "plt.title(\"we notice that the IQRs for these images are small!\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxSRUEI9lW6"
      },
      "source": [
        "# Adversarial image generation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-XNTfmy6nz3"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Mgai4q9eZu"
      },
      "source": [
        "print (std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6vhHdRxEDrp"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "images, labels = next(iter(ds_train))\n",
        "labels_class = tf.argmax(labels, axis=1)\n",
        "\n",
        "attack = fb.attacks.FGSM()\n",
        "fmodel = fb.models.TensorFlowModel(model=pretrained_model, bounds=(bound_min, bound_max))\n",
        "\n",
        "fimages = attack.run(fmodel, images, criterion=fb.criteria.Misclassification(labels_class), epsilon=0.1)\n",
        "fpreds = pretrained_model.predict(fimages)\n",
        "\n",
        "fpreds_class = tf.argmax(fpreds, axis=1)\n",
        "\n",
        "# accuracy before attack\n",
        "pretrained_model.evaluate(images,labels)\n",
        "# accuracy after attack\n",
        "pretrained_model.evaluate(fimages,labels)\n",
        "\n",
        "nb=0\n",
        "nb_samples=1\n",
        "correct=0\n",
        "for img, fimg, label, fpred in zip(images, fimages, labels_class, fpreds_class): \n",
        "  if label==fpred: \n",
        "    correct+=1\n",
        "  plt.figure(figsize = (2,1))\n",
        "  fig, ax = plt.subplots(1,2)\n",
        "  ax[0].imshow((img.numpy() * std + mean).astype(np.uint8))\n",
        "  ax[0].set_title(class_names[label])\n",
        "  ax[0].axis('off')\n",
        "  ax[1].imshow((fimg.numpy() * std + mean).astype(np.uint8))\n",
        "  ax[1].set_title(class_names[fpred])\n",
        "  ax[1].axis('off')\n",
        "  plt.show()\n",
        "  if nb>nb_samples: \n",
        "    break\n",
        "  nb+=1\n",
        "print(correct)\n",
        "  # if input()==\"q\": \n",
        "  #   break \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnEJHsaYrXq_"
      },
      "source": [
        "images, labels = next(iter(ds_train))\n",
        "labels_class = tf.argmax(labels, axis=1)\n",
        "\n",
        "attack = fb.attacks.FGSM()\n",
        "fmodel = fb.models.TensorFlowModel(model=pretrained_model, bounds=(bound_min, bound_max))\n",
        "\n",
        "fimages = attack.run(fmodel, images, criterion=fb.criteria.Misclassification(labels_class), epsilon=0.1)\n",
        "\n",
        "# accuracy before attack\n",
        "pretrained_model.evaluate(images,labels)\n",
        "# accuracy after attack\n",
        "pretrained_model.evaluate(fimages,labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjv6jKQi9zEX"
      },
      "source": [
        "# IQR calculations for adversarial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSgufDWOdjHK"
      },
      "source": [
        "%%time \n",
        "# calculate the IQR for the adversarial images \n",
        "# compare with the IQR of original \n",
        "fiqr = [] \n",
        "\n",
        "fpreds = pretrained_model.predict(fimages)\n",
        "fpreds_value = tf.reduce_max(fpreds, axis=1)\n",
        "\n",
        "for i in range(32): \n",
        "  for j in range(32): \n",
        "    mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "    mask[:,i,j,:]=0 \n",
        "    fimages_0 = fimages * mask\n",
        "    fpreds_0 = pretrained_model.predict(fimages_0)\n",
        "    fpreds_value_0 = tf.reduce_max(fpreds_0, axis=1)\n",
        "    fiqr.append(abs(fpreds_value - fpreds_value_0))\n",
        "\n",
        "fiqr = np.array(fiqr)\n",
        "print(fiqr.shape) \n",
        "fiqr_vals = np.percentile(fiqr, 75, axis=0)-np.percentile(fiqr, 25, axis=0)\n",
        "\n",
        "print (fiqr_vals.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqA-nxCQMww"
      },
      "source": [
        "plt.scatter(range(BATCH_SIZE), iqr_vals, label='oig')\n",
        "plt.scatter(range(BATCH_SIZE), fiqr_vals, label='adv', marker='s')\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"IQR-1D: Orig. vs. FGSM ($\\epsilon=0.1$) Adv.\")\n",
        "plt.xlabel(\"images\")\n",
        "plt.ylabel(\"IQR\")\n",
        "plt.show()\n",
        "# It looks like in general orig has lower IQR than ADV "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMPXZdPodykQ"
      },
      "source": [
        "# Classification Adv. vs. Orig\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md_iI3WExCTY"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "# We try a very basic classification \n",
        "# print (type(fiqr_vals))\n",
        "X = np.concatenate((iqr_vals, fiqr_vals), axis=0)\n",
        "y = np.concatenate( ( np.zeros(iqr_vals.shape), np.ones(fiqr_vals.shape) ) )\n",
        "\n",
        "X,y = shuffle(X,y)\n",
        "# print (y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qybonYWQQgRj"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "\n",
        "score = cross_val_score(XGBClassifier(), X.reshape(-1,1), y, cv=2)\n",
        "print (score)\n",
        "\n",
        "score = cross_val_score(SVC(), X.reshape(-1,1), y, cv=2)\n",
        "print (score)\n",
        "\n",
        "# we have a little better than random accuracy "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfT_XFMLr5w-"
      },
      "source": [
        "# IQR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeFd_xX9W-ti"
      },
      "source": [
        "# now what if we enhance our classification accuracy by adding more IQR values \n",
        "# the IQR values correspond to random nodes in the network\n",
        "\n",
        "# let's start with the 10 output nodes. We call it iqr-10\n",
        "\n",
        "%%time \n",
        "\n",
        "\n",
        "NB_BATCHES=1\n",
        "nb=1\n",
        "for images, labels in ds_train: \n",
        "  preds = pretrained_model.predict(images)\n",
        "  iqr=[]\n",
        "  for i in range(32): \n",
        "    for j in range(32): \n",
        "      mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "      mask[:,i,j,:]=0 \n",
        "      images_0 = images * mask\n",
        "      preds_0 = pretrained_model.predict(images_0)\n",
        "      iqr.append(abs(preds - preds_0))\n",
        "  \n",
        "  if nb==1: \n",
        "    iqr_all = np.array(iqr) \n",
        "    # print(iqr_all.shape)\n",
        "  else: \n",
        "    iqr = np.array(iqr) \n",
        "    # print(iqr.shape)\n",
        "    # print(iqr_all.shape)\n",
        "    iqr_all = np.concatenate((iqr_all, iqr), axis=1)      \n",
        "    # print(iqr_all.shape)\n",
        "    \n",
        "  print(nb) \n",
        "  nb+=1\n",
        "  if nb > NB_BATCHES:\n",
        "    break  \n",
        " \n",
        "\n",
        "\n",
        "print(iqr_all.shape) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOigF0xnvpvC"
      },
      "source": [
        "iqr_vals = np.percentile(iqr_all, 75, axis=0)-np.percentile(iqr_all, 25, axis=0)\n",
        "print (iqr_vals.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPm9sd5CSOxo"
      },
      "source": [
        "# IQR 10 for adversarial \n",
        "%%time\n",
        "\n",
        "nb=1\n",
        "\n",
        "attack = fb.attacks.FGSM()\n",
        "fmodel = fb.models.TensorFlowModel(model=pretrained_model, bounds=(bound_min, bound_max))\n",
        "\n",
        "\n",
        "for images, labels in ds_train:\n",
        "  \n",
        "  labels_class = tf.argmax(labels, axis=1)\n",
        "  fimages = attack.run(fmodel, images, criterion=fb.criteria.Misclassification(labels_class), epsilon=0.1)\n",
        "  fpreds = pretrained_model.predict(fimages)\n",
        "  pretrained_model.evaluate(fimages,labels)\n",
        "  fiqr = [] \n",
        "  for i in range(32): \n",
        "    for j in range(32): \n",
        "      mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "      mask[:,i,j,:]=0 \n",
        "      fimages_0 = fimages * mask\n",
        "      fpreds_0 = pretrained_model.predict(fimages_0)\n",
        "      fiqr.append(abs(fpreds - fpreds_0))\n",
        "  \n",
        "\n",
        "  \n",
        "  if nb==1: \n",
        "    fiqr_all = np.array(fiqr)\n",
        "  else: \n",
        "    fiqr = np.array(fiqr)\n",
        "    fiqr_all = np.concatenate((fiqr_all, fiqr), axis=1)\n",
        "\n",
        "  print(nb) \n",
        "  nb+=1\n",
        "  if nb > NB_BATCHES:\n",
        "    break \n",
        "\n",
        "print (fiqr_all.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H71wARAuxvt_"
      },
      "source": [
        "fiqr_vals = np.percentile(fiqr_all, 75, axis=0)-np.percentile(fiqr_all, 25, axis=0)\n",
        "print (fiqr_vals.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip2RLlCNRw_j"
      },
      "source": [
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 5, figsize=(20, 6), sharex=True, sharey=True)\n",
        "\n",
        "\n",
        "j=0\n",
        "for i in range(10):\n",
        "  axs[j,i-j*5].scatter(range(BATCH_SIZE*NB_BATCHES), iqr_vals[:,i], label='oig')\n",
        "  axs[j,i-j*5].scatter(range(BATCH_SIZE*NB_BATCHES), fiqr_vals[:,i], label='adv', marker='s')\n",
        "  axs[j,i-j*5].legend(loc=2)\n",
        "  # axs[j,i-j*5].set(xlabel='images', ylabel='IQR')\n",
        "  # axs[j,j*5+i].ylabel(\"IQR\")\n",
        "  if i==4: \n",
        "    j+=1\n",
        "plt.suptitle(\"IQR-10D: Orig. vs. FGSM ($\\epsilon=0.1$) Adv.\", fontsize=16)\n",
        "\n",
        "\n",
        "\n",
        "fig.text(0.5, 0.04, 'Images', ha='center', fontweight='bold')\n",
        "fig.text(0.09, 0.5, 'IQR', va='center', rotation='vertical', fontweight='bold')\n",
        "# plt.ylabel(\"IQR\")\n",
        "# plt.xlabel(\"images\")\n",
        "# fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CirSyPIUQqA"
      },
      "source": [
        "\n",
        "X = np.concatenate((iqr_vals, fiqr_vals), axis=0)\n",
        "y = np.concatenate( ( np.zeros(iqr_vals.shape[0]), np.ones(fiqr_vals.shape[0]) ) )\n",
        "print(X.shape)\n",
        "print (y.shape)\n",
        "score = cross_val_score(XGBClassifier(), X, y, cv=3)\n",
        "print (sum(score)/3)\n",
        "\n",
        "score = cross_val_score(SVC(C=10**5, gamma=10), X, y, cv=3)\n",
        "print (sum(score)/3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o9RZ65FyM_Z"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameters = {'C':[0.1, 1, 10, 100, 1000, 10000, 10**5], 'gamma':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
        "svc = SVC()\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "clf.fit(X,y)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "# print (clf.best_estimator_, clf.best_index_)\n",
        "score = cross_val_score(clf.best_estimator_, X, y, cv=3)\n",
        "print (sum(score)/3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnwrdseoW_hy"
      },
      "source": [
        "# ML-LOO: Mutli Layer approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3EGDWvVk23a"
      },
      "source": [
        "# pretrained_model.summary()\n",
        "NB_LAYERS=20\n",
        "NB_NODES_PER_LAYER=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8YD4IC8NVce"
      },
      "source": [
        "Example for an intermediate model in Kears\n",
        "\n",
        "Source: https://stackoverflow.com/questions/41711190/keras-how-to-get-the-output-of-each-layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iQK7jZQlyPf"
      },
      "source": [
        "# code to print any intermediate node from any intermediate layer \n",
        "from keras import backend as K\n",
        "\n",
        "# inp = pretrained_model.input  \n",
        "# # last two layers \n",
        "# outputs = [layer.output for layer in pretrained_model.layers[-2:]]          \n",
        "# functor = K.function([inp], outputs) \n",
        "# Testing\n",
        "# test = np.random.random((32,32,3))[np.newaxis,...]\n",
        "# layer_outs = functor([test])\n",
        "# print(layer_outs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Biy7YfpiHE"
      },
      "source": [
        "\n",
        "\n",
        "inp = pretrained_model.input  \n",
        "layers_= pretrained_model.layers[-NB_LAYERS:]\n",
        "outputs = [lay.output for lay in layers_]     \n",
        "intermediate_model = K.function([inp], outputs) \n",
        "\n",
        "\n",
        "# select NB_NODES_PER_LAYER random nodes from each selected layer\n",
        "print (\"these nodes will be used to compute the IQR-\"+str(NB_LAYERS*NB_NODES_PER_LAYER))\n",
        "\n",
        "node_indices=[]\n",
        "for lay in layers_[:-1]:\n",
        "  # we omit the first dim (batch dim) of each layer \n",
        "  node_indices.append([[np.random.randint(0,d) for d in lay.output.shape[1:]] for s in range(NB_NODES_PER_LAYER)])\n",
        "  print(\"%s:\" % lay.name)\n",
        "  \n",
        "\n",
        "# add the last layer \n",
        "node_indices.append([[x] for x in range(10)])\n",
        "# print(\"%s:\" % layers_[-1].name)\n",
        "print(len(node_indices)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shY8j3ffA0QW"
      },
      "source": [
        "# collect values for a batch example \n",
        "images, labels = next(iter(ds_train))\n",
        "\n",
        "preds_layers = intermediate_model(images)\n",
        "\n",
        "preds=[]\n",
        "for i in range(NB_LAYERS): # loop through the last NB_LAYERS layers \n",
        "  print (preds_layers[i].shape) \n",
        "  for j in node_indices[i]: # loop through the NB_NODES_PER_LAYER random nodes for that layer \n",
        "    t = tuple(j)\n",
        "    # print((0,*t))\n",
        "    # preds.append(preds_layers[i])\n",
        "    preds.append ( preds_layers[i][(...,*t)] )\n",
        "\n",
        "preds = np.array(preds).T\n",
        "print (preds.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4-Qilcq9Vao"
      },
      "source": [
        "# now what if we enhance our classification accuracy by adding more IQR values \n",
        "# the IQR values correspond to random nodes in the network\n",
        "\n",
        "# let' select 100 output nodes from each layer. We call it iqr-1000 (it is actually iqr-910 since last layer is only 10)\n",
        "\n",
        "%%time \n",
        "\n",
        "\n",
        "NB_BATCHES=2\n",
        "nb=1\n",
        "\n",
        "for images, labels in ds_train: \n",
        "  # compute the IQR original for the batch\n",
        "  preds_layers = intermediate_model(images)\n",
        "  preds = [] \n",
        "  for i in range(NB_LAYERS): # loop through the last NB_LAYERS\n",
        "    for j in node_indices[i]: # loop through the NB_NODES_PER_LAYER\n",
        "      t = tuple(j)\n",
        "      # print((0,*t))\n",
        "      # preds.append(preds_layers[i])\n",
        "      preds.append ( preds_layers[i][(...,*t)] )\n",
        "  preds = np.array(preds).T\n",
        "  \n",
        "  iqr=[]\n",
        "  for i in range(32): \n",
        "    for j in range(32): \n",
        "      mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "      mask[:,i,j,:]=0 \n",
        "      images_0 = images * mask\n",
        "      preds_layers_0 = intermediate_model(images_0)\n",
        "      preds_0 = [] \n",
        "      for u in range(NB_LAYERS): # loop through the last 10 layers \n",
        "        for v in node_indices[u]: # loop through the 10 random nodes for that layer \n",
        "          t = tuple(v)\n",
        "          # print((0,*t))\n",
        "          # preds.append(preds_layers[i])\n",
        "          preds_0.append ( preds_layers_0[u][(...,*t)] )\n",
        "      preds_0 = np.array(preds_0).T\n",
        "      iqr.append(abs(preds - preds_0))\n",
        "  \n",
        "  iqr = np.array(iqr)\n",
        "  # print(iqr.shape)\n",
        "  iqr_vals_batch = np.percentile(iqr, 75, axis=0)-np.percentile(iqr, 25, axis=0)\n",
        "  # print(iqr_vals_batch.shape)\n",
        "  if nb==1: \n",
        "    iqr_vals = iqr_vals_batch\n",
        "    # print(iqr_all.shape)\n",
        "  else: \n",
        "    iqr_vals = np.concatenate((iqr_vals, iqr_vals_batch), axis=0)      \n",
        "\n",
        "    \n",
        "  print(nb) \n",
        "  nb+=1\n",
        "  if nb > NB_BATCHES:\n",
        "    break  \n",
        " \n",
        "\n",
        "\n",
        "print(iqr_vals.shape) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvsOOk-mIJVL"
      },
      "source": [
        "from sys import getsizeof\n",
        "print (iqr.shape)\n",
        "getsizeof(iqr)/10**9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U6LLrTIKptH"
      },
      "source": [
        "# iqr_vals = np.percentile(iqr_all, 75, axis=0)-np.percentile(iqr_all, 25, axis=0)\n",
        "print (iqr_vals.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KLZexqDLN7H"
      },
      "source": [
        "# IQR 3810 for adversarial \n",
        "%%time\n",
        "\n",
        "nb=1\n",
        "\n",
        "attack = fb.attacks.FGSM()\n",
        "fmodel = fb.models.TensorFlowModel(model=pretrained_model, bounds=(bound_min, bound_max))\n",
        "\n",
        "\n",
        "for images, labels in ds_train:\n",
        "  \n",
        "  labels_class = tf.argmax(labels, axis=1)\n",
        "  fimages = attack.run(fmodel, images, criterion=fb.criteria.Misclassification(labels_class), epsilon=0.1)\n",
        "  fpreds_layers = intermediate_model(fimages)\n",
        "  fpreds = [] \n",
        "  for i in range(NB_LAYERS): # loop through the last 10 layers ]\n",
        "    for j in node_indices[i]: # loop through the 10 random nodes for that layer \n",
        "      t = tuple(j)\n",
        "      fpreds.append ( fpreds_layers[i][(...,*t)] )\n",
        "  fpreds = np.array(fpreds).T\n",
        "  pretrained_model.evaluate(fimages,labels)\n",
        "  \n",
        "  fiqr = [] \n",
        "  for i in range(32): \n",
        "    for j in range(32): \n",
        "      mask = np.ones((BATCH_SIZE,32,32,3)) \n",
        "      mask[:,i,j,:]=0 \n",
        "      fimages_0 = fimages * mask\n",
        "      fpreds_layers_0 = intermediate_model(fimages_0)\n",
        "      fpreds_0 = [] \n",
        "      for u in range(NB_LAYERS): # loop through the last 10 layers ]\n",
        "        for v in node_indices[u]: # loop through the 10 random nodes for that layer \n",
        "          t = tuple(v)\n",
        "          fpreds_0.append ( fpreds_layers_0[u][(...,*t)] )\n",
        "      fpreds_0 = np.array(fpreds_0).T\n",
        "      fiqr.append(abs(fpreds - fpreds_0))\n",
        "  fiqr = np.array(fiqr)\n",
        "  fiqr_vals_batch = np.percentile(fiqr, 75, axis=0)-np.percentile(fiqr, 25, axis=0)\n",
        "  # batch management\n",
        "  if nb==1: \n",
        "    fiqr_vals = fiqr_vals_batch\n",
        "  else: \n",
        "    fiqr_vals = np.concatenate((fiqr_vals, fiqr_vals_batch), axis=0)\n",
        "\n",
        "  print(nb) \n",
        "  nb+=1\n",
        "  if nb > NB_BATCHES:\n",
        "    break \n",
        "\n",
        "print (fiqr_vals.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N764OVQIMsLM"
      },
      "source": [
        "# fiqr_vals = np.percentile(fiqr_all, 75, axis=0)-np.percentile(fiqr_all, 25, axis=0)\n",
        "# print (fiqr_vals.shape) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHZVQxj4XUTV"
      },
      "source": [
        "# free ram memory\n",
        "# del(fiqr_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIvMCGAJLBDA"
      },
      "source": [
        "X = np.concatenate((iqr_vals, fiqr_vals), axis=0)\n",
        "y = np.concatenate( ( np.zeros(iqr_vals.shape[0]), np.ones(fiqr_vals.shape[0]) ) )\n",
        "print(X.shape)\n",
        "print (y.shape)\n",
        "score = np.average(cross_val_score(XGBClassifier(), X, y, cv=3)) \n",
        "print (score)\n",
        "\n",
        "\n",
        "score = np.average(cross_val_score(SVC(), X, y, cv=3))\n",
        "print (score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSRyuMkALnAW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "parameters = {'C':[0.1, 1, 10, 100, 1000, 10000, 10**5], 'gamma':[0.0001, 0.001, 0.01, 0.1, 1, 10]}\n",
        "svc = SVC()\n",
        "clf = GridSearchCV(svc, parameters)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "clf.fit(X_train,y_train)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "# print (clf.best_estimator_, clf.best_index_)\n",
        "clf.best_estimator_.score(X_test, y_test)\n",
        "# score = cross_val_score(clf.best_estimator_, X, y, cv=3)\n",
        "# print (sum(score)/3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDw-mb5dt3M"
      },
      "source": [
        "SVC().fit(X,y).score(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVXp3ILPeDww"
      },
      "source": [
        "# Segmentation Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDk8iQUpr00h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}